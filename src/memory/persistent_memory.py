"""\nPersistent Memory System with RAG and Vector Storage\nSupports long-term, short-term, and episodic memory across sessions\n"""\n\nimport os\nimport json\nimport pickle\nfrom datetime import datetime\nfrom typing import List, Dict, Any, Optional\nfrom dataclasses import dataclass, asdict\nimport chromadb\nfrom chromadb.config import Settings\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer\n\n\n@dataclass\nclass MemoryEntry:\n    """Structured memory entry"""\n    id: str\n    content: str\n    timestamp: datetime\n    memory_type: str  # short_term, long_term, episodic, semantic\n    embedding: Optional[List[float]] = None\n    metadata: Dict[str, Any] = None\n    importance_score: float = 0.5\n    access_count: int = 0\n    last_accessed: Optional[datetime] = None\n\n\nclass PersistentMemoryManager:\n    """\n    Enterprise-grade memory management with multi-layered storage\n    - Vector database (ChromaDB) for semantic search\n    - Traditional DB for structured data\n    - File system for large objects\n    - Redis-like caching layer\n    """\n    \n    def __init__(self, base_path: str = "./memory_store"):\n        self.base_path = base_path\n        os.makedirs(base_path, exist_ok=True)\n        \n        # Initialize embedding model\n        self.embedder = SentenceTransformer('all-MiniLM-L6-v2')\n        \n        # Initialize vector database\n        self.chroma_client = chromadb.Client(Settings(\n            chroma_db_impl="duckdb+parquet",\n            persist_directory=os.path.join(base_path, "chroma_db")\n        ))\n        \n        # Create collections for different memory types\n        self.short_term_collection = self._get_or_create_collection("short_term_memory")\n        self.long_term_collection = self._get_or_create_collection("long_term_memory")\n        self.episodic_collection = self._get_or_create_collection("episodic_memory")\n        self.semantic_collection = self._get_or_create_collection("semantic_memory")\n        \n        # Session management\n        self.current_session_id = self._generate_session_id()\n        self.session_memory = []\n        \n    def _get_or_create_collection(self, name: str):\n        """Get or create a ChromaDB collection"""\n        try:\n            return self.chroma_client.get_collection(name)\n        except:\n            return self.chroma_client.create_collection(name)\n    \n    def _generate_session_id(self) -> str:\n        """Generate unique session ID"""\n        return f"session_{datetime.now().strftime('%Y%m%d_%H%M%S')}"\n    \n    def add_memory(self, \n                   content: str, \n                   memory_type: str = "short_term",\n                   metadata: Dict[str, Any] = None,\n                   importance: float = 0.5) -> str:\n        """\n        Add a new memory entry with automatic embedding and storage\n        """\n        # Generate embedding\n        embedding = self.embedder.encode(content).tolist()\n        \n        # Create memory entry\n        entry = MemoryEntry(\n            id=f"{memory_type}_{datetime.now().timestamp()}",\n            content=content,\n            timestamp=datetime.now(),\n            memory_type=memory_type,\n            embedding=embedding,\n            metadata=metadata or {},\n            importance_score=importance\n        )\n        \n        # Store in appropriate collection\n        collection = self._get_collection_by_type(memory_type)\n        collection.add(\n            ids=[entry.id],\n            embeddings=[embedding],\n            documents=[content],\n            metadatas=[{"timestamp": entry.timestamp.isoformat(), \n                       "importance": importance,\n                       "session_id": self.current_session_id}]\n        )\n        \n        # Add to session memory\n        self.session_memory.append(entry)\n        \n        return entry.id\n    \n    def retrieve_similar(self, \n                        query: str, \n                        memory_type: str = None,\n                        n_results: int = 5) -> List[Dict]:\n        """\n        Retrieve similar memories using semantic search\n        """\n        query_embedding = self.embedder.encode(query).tolist()\n        \n        if memory_type:\n            collections = [self._get_collection_by_type(memory_type)]\n        else:\n            # Search across all collections\n            collections = [\n                self.short_term_collection,\n                self.long_term_collection,\n                self.episodic_collection,\n                self.semantic_collection\n            ]\n        \n        all_results = []\n        for collection in collections:\n            results = collection.query(\n                query_embeddings=[query_embedding],\n                n_results=n_results\n            )\n            \n            if results['documents']:\n                for i, doc in enumerate(results['documents'][0]):\n                    all_results.append({\n                        'content': doc,\n                        'distance': results['distances'][0][i] if 'distances' in results else None,\n                        'metadata': results['metadatas'][0][i] if 'metadatas' in results else {}\n                    })\n        \n        # Sort by relevance (distance)\n        all_results.sort(key=lambda x: x['distance'] if x['distance'] else float('inf'))\n        \n        return all_results[:n_results]\n    \n    def consolidate_memory(self):\n        """\n        Move important short-term memories to long-term storage\n        Implement memory consolidation similar to human sleep\n        """\n        # Query high-importance short-term memories\n        short_term_results = self.short_term_collection.get()\n        \n        if not short_term_results['ids']:\n            return\n        \n        for i, memory_id in enumerate(short_term_results['ids']):\n            metadata = short_term_results['metadatas'][i]\n            \n            # Move high-importance memories to long-term\n            if metadata.get('importance', 0) > 0.7:\n                self.long_term_collection.add(\n                    ids=[f"lt_{memory_id}"],\n                    embeddings=[short_term_results['embeddings'][i]] if short_term_results.get('embeddings') else None,\n                    documents=[short_term_results['documents'][i]],\n                    metadatas=[metadata]\n                )\n    \n    def get_context_window(self, query: str, window_size: int = 10) -> str:\n        """\n        Get relevant context for a query by combining multiple memory types\n        """\n        context_parts = []\n        \n        # Get semantic memories (facts, knowledge)\n        semantic = self.retrieve_similar(query, "semantic", n_results=3)\n        if semantic:\n            context_parts.append("### Relevant Knowledge:")\n            for mem in semantic:\n                context_parts.append(f"- {mem['content']}")\n        \n        # Get episodic memories (past conversations)\n        episodic = self.retrieve_similar(query, "episodic", n_results=3)\n        if episodic:\n            context_parts.append("\n### Past Interactions:")\n            for mem in episodic:\n                context_parts.append(f"- {mem['content']}")\n        \n        # Get recent short-term context\n        if self.session_memory:\n            context_parts.append("\n### Current Session Context:")\n            for entry in self.session_memory[-window_size:]:\n                context_parts.append(f"- {entry.content}")\n        \n        return "\n".join(context_parts)\n    \n    def _get_collection_by_type(self, memory_type: str):\n        """Get the appropriate collection for a memory type"""\n        collections = {\n            "short_term": self.short_term_collection,\n            "long_term": self.long_term_collection,\n            "episodic": self.episodic_collection,\n            "semantic": self.semantic_collection\n        }\n        return collections.get(memory_type, self.short_term_collection)\n    \n    def save_session(self):\n        """Save current session to disk"""\n        session_file = os.path.join(self.base_path, f"{self.current_session_id}.pkl")\n        with open(session_file, 'wb') as f:\n            pickle.dump(self.session_memory, f)\n    \n    def load_previous_sessions(self, n_sessions: int = 5) -> List[MemoryEntry]:\n        """Load memories from previous sessions"""\n        session_files = sorted([\n            f for f in os.listdir(self.base_path) \n            if f.endswith('.pkl')\n        ], reverse=True)[:n_sessions]\n        \n        memories = []\n        for session_file in session_files:\n            with open(os.path.join(self.base_path, session_file), 'rb') as f:\n                memories.extend(pickle.load(f))\n        \n        return memories
